{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_text</th>\n",
       "      <th>short_answers</th>\n",
       "      <th>title</th>\n",
       "      <th>paragraphs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>which is the most common use of opt-in e-mail ...</td>\n",
       "      <td>a newsletter sent to an advertising firm 's cu...</td>\n",
       "      <td>Email marketing</td>\n",
       "      <td>[Email marketing - Wikipedia  Email marketing ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>how i.met your mother who is the mother</td>\n",
       "      <td>Tracy McConnell</td>\n",
       "      <td>The Mother ( How I Met Your Mother )</td>\n",
       "      <td>[The Mother ( How I Met Your Mother ) - wikipe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>who had the most wins in the nfl</td>\n",
       "      <td>Tom Brady</td>\n",
       "      <td>List of National Football League career quarte...</td>\n",
       "      <td>[List of National Football League career quart...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>who played mantis guardians of the galaxy 2</td>\n",
       "      <td>Pom Klementieff</td>\n",
       "      <td>Pom Klementieff</td>\n",
       "      <td>[Pom Klementieff - wikipedia  Pom Klementieff ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the nashville sound brought a polished and cos...</td>\n",
       "      <td>the use of lush string arrangements with a rea...</td>\n",
       "      <td>Nashville sound</td>\n",
       "      <td>[Nashville sound - wikipedia  Nashville sound ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       question_text  \\\n",
       "0  which is the most common use of opt-in e-mail ...   \n",
       "1            how i.met your mother who is the mother   \n",
       "2                   who had the most wins in the nfl   \n",
       "3        who played mantis guardians of the galaxy 2   \n",
       "4  the nashville sound brought a polished and cos...   \n",
       "\n",
       "                                       short_answers  \\\n",
       "0  a newsletter sent to an advertising firm 's cu...   \n",
       "1                                    Tracy McConnell   \n",
       "2                                          Tom Brady   \n",
       "3                                    Pom Klementieff   \n",
       "4  the use of lush string arrangements with a rea...   \n",
       "\n",
       "                                               title  \\\n",
       "0                                    Email marketing   \n",
       "1               The Mother ( How I Met Your Mother )   \n",
       "2  List of National Football League career quarte...   \n",
       "3                                    Pom Klementieff   \n",
       "4                                    Nashville sound   \n",
       "\n",
       "                                          paragraphs  \n",
       "0  [Email marketing - Wikipedia  Email marketing ...  \n",
       "1  [The Mother ( How I Met Your Mother ) - wikipe...  \n",
       "2  [List of National Football League career quart...  \n",
       "3  [Pom Klementieff - wikipedia  Pom Klementieff ...  \n",
       "4  [Nashville sound - wikipedia  Nashville sound ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ast import literal_eval\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('dataset.csv', converters={'paragraphs': literal_eval})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nlp(question=\"What is the mother in how i met your mother?\", context=\"\".join(df['paragraphs'][1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
    "\n",
    "text = df['paragraphs'][3]\n",
    "\n",
    "questions = [\n",
    "    \"who played mantis guardians of the galaxy 2\",\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    inputs = tokenizer.encode_plus(question, text, add_special_tokens=True, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].tolist()[0]\n",
    "\n",
    "    text_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "    answer_start_scores, answer_end_scores = model(**inputs)\n",
    "\n",
    "    answer_start = torch.argmax(\n",
    "        answer_start_scores\n",
    "    )  # Get the most likely beginning of answer with the argmax of the score\n",
    "    answer_end = torch.argmax(answer_end_scores) + 1  # Get the most likely end of answer with the argmax of the score\n",
    "\n",
    "    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n",
    "\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {answer}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"deepset/roberta-base-squad2\")\n",
    "\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"deepset/roberta-base-squad2\")\n",
    "text = df['paragraphs'][3]\n",
    "\n",
    "questions = [\n",
    "    \"who played mantis guardians of the galaxy 2\",\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    inputs = tokenizer.encode_plus(question, text, add_special_tokens=True, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].tolist()[0]\n",
    "\n",
    "    text_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "    answer_start_scores, answer_end_scores = model(**inputs)\n",
    "\n",
    "    answer_start = torch.argmax(\n",
    "        answer_start_scores\n",
    "    )  # Get the most likely beginning of answer with the argmax of the score\n",
    "    answer_end = torch.argmax(answer_end_scores) + 1  # Get the most likely end of answer with the argmax of the score\n",
    "\n",
    "    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n",
    "\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {answer}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QuestionAnsweringPipeline\n",
    "classtransformers.QuestionAnsweringPipeline(model: Union[PreTrainedModel, TFPreTrainedModel], tokenizer: transformers.tokenization_utils.PreTrainedTokenizer, modelcard: Optional[transformers.modelcard.ModelCard] = None, framework: Optional[str] = None, device: int = - 1, task: str = '', **kwargs)[SOURCE]\n",
    "Question Answering pipeline using ModelForQuestionAnswering head. See the question answering usage examples for more information.\n",
    "\n",
    "This question answering can currently be loaded from the pipeline() method using the following task identifier(s):\n",
    "\n",
    "“question-answering”, for answering questions given a context.\n",
    "\n",
    "The models that this pipeline can use are models that have been fine-tuned on a question answering task. See the up-to-date list of available models on huggingface.co/models.\n",
    "\n",
    "Parameters\n",
    "model (PreTrainedModel or TFPreTrainedModel) – The model that will be used by the pipeline to make predictions. This needs to be a model inheriting from PreTrainedModel for PyTorch and TFPreTrainedModel for TensorFlow.\n",
    "\n",
    "tokenizer (PreTrainedTokenizer) – The tokenizer that will be used by the pipeline to encode data for the model. This object inherits from PreTrainedTokenizer.\n",
    "\n",
    "modelcard (str or ModelCard, optional, defaults to None) – Model card attributed to the model for this pipeline.\n",
    "\n",
    "framework (str, optional, defaults to None) –\n",
    "\n",
    "The framework to use, either “pt” for PyTorch or “tf” for TensorFlow. The specified framework must be installed.\n",
    "\n",
    "If no framework is specified, will default to the one currently installed. If no framework is specified and both frameworks are installed, will default to PyTorch.\n",
    "\n",
    "args_parser (ArgumentHandler, optional, defaults to None) – Reference to the object in charge of parsing supplied pipeline parameters.\n",
    "\n",
    "device (int, optional, defaults to -1) – Device ordinal for CPU/GPU supports. Setting this to -1 will leverage CPU, >=0 will run the model on the associated CUDA device id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import pipeline\n",
    "\n",
    "# nlp = pipeline(\"question-answering\")\n",
    "\n",
    "# context = r\"\"\"\n",
    "# Extractive Question Answering is the task of extracting an answer from a text given a question. An example of a\n",
    "# question answering dataset is the SQuAD dataset, which is entirely based on that task. If you would like to fine-tune\n",
    "# a model on a SQuAD task, you may leverage the `run_squad.py`.\n",
    "# \"\"\"\n",
    "\n",
    "# print(nlp(question=\"What is extractive question answering?\", context=context))\n",
    "# print(nlp(question=\"What is a good example of a question answering dataset?\", context=context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a857ee4ec3c4c0e8f002bd66bc6840e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=230.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "nlp_bert = pipeline(\"question-answering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  who played mantis guardians of the galaxy 2\n",
      "Annotated answer:  Pom Klementieff\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "convert squad examples to features: 100%|██████████| 1/1 [00:00<00:00, 348.39it/s]\n",
      "add example index and unique id: 100%|██████████| 1/1 [00:00<00:00, 5426.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.0035539168985547764, 'start': 0, 'end': 15, 'answer': 'Pom Klementieff'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Question: \", df['question_text'][3])\n",
    "print(\"Annotated answer: \", df['short_answers'][3])\n",
    "print(nlp(question=df['question_text'][3], context=df['paragraphs'][3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  how i.met your mother who is the mother\n",
      "Annotated answer:  Tracy McConnell\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "convert squad examples to features: 100%|██████████| 1/1 [00:00<00:00,  3.83it/s]\n",
      "add example index and unique id: 100%|██████████| 1/1 [00:00<00:00, 5236.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.4352570122007826, 'start': 50, 'end': 60, 'answer': 'The Mother'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Question: \", df['question_text'][1])\n",
    "print(\"Annotated answer: \", df['short_answers'][1])\n",
    "\n",
    "print(nlp(question=\"What is the mother in how i met your mother?\", context=\"\".join(df['paragraphs'][1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-8-1d4e8c9a1d0a>, line 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-8-1d4e8c9a1d0a>\"\u001b[0;36m, line \u001b[0;32m11\u001b[0m\n\u001b[0;31m    'context': \"\".join(df['paragraphs'][1]}\u001b[0m\n\u001b[0m                                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from transformers.pipelines import pipeline\n",
    "from transformers.modeling_auto import AutoModelForQuestionAnswering\n",
    "from transformers.tokenization_auto import AutoTokenizer\n",
    "\n",
    "model_name = \"deepset/roberta-base-squad2\"\n",
    "\n",
    "# a) Get predictions\n",
    "nlp = pipeline('question-answering', model=model_name, tokenizer=model_name)\n",
    "QA_input = {\n",
    "    'question': df['question_text'][1],\n",
    "    'context': \"\".join(df['paragraphs'][1]\n",
    "}\n",
    "res = nlp(QA_input)\n",
    "\n",
    "# b) Load model & tokenizer\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_aws_neuron_pytorch_p36)",
   "language": "python",
   "name": "conda_aws_neuron_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
